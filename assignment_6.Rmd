---
title: "Assignment 6"
output: word_document
date: "2023-02-27"
---

```{r setup, message=FALSE, warning=FALSE}
library(tidyverse)
library(rpart)
library(caret)
library(rpart.plot)
library(pROC)
library(NHANES)


knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)


```

## Restricting the NHANES data to the list of 11 variables below and Partitioning the data into training and testing using a 70/30 split.

## "Age", "Race1", "Education", "HHIncome", "Weight", "Height", "Pulse", "Diabetes", "BMI", "PhysActive", "Smoke100"

```{r}
set.seed(123)
hw6.df=NHANES %>% 
  janitor::clean_names() %>% 
  select( age, race1, education, hh_income, weight, height, pulse, diabetes, bmi, phys_active, smoke100 ) 

hw6 <-hw6.df[complete.cases(hw6.df),]

summary(hw6$diabetes)

train.indices<-createDataPartition(y=hw6$diabetes,p=0.7,list=FALSE)
train.data<-hw6[train.indices, ]
test.data<-hw6[-train.indices, ]


```

## Constructing a classification tree model within the training data set


```{r classtree}

set.seed(123)

### Creating 10-fold cross-validation and using down-sampling because of imbalance in data
train.control<-trainControl(method="cv", number=10, sampling="down")

### Create sequence of cp parameters to try 
grid.2<-expand.grid(cp=seq(0.001, 0.3, by=0.01))

#### Train model
tree.diabetes<-train(diabetes~., data=train.data, method="rpart",trControl=train.control, tuneGrid=grid.2)
tree.diabetes$results
tree.diabetes$bestTune

### PLotting the classification tree
rpart.plot(tree.diabetes$finalModel)

### Obtaining variable importance on the final model within training data
varImp(tree.diabetes)

### Getting accuracy metric and confusion matrix from training.
confusionMatrix(tree.diabetes)

```
 * The Best Tuned (Final) model had a CP of 0.001 and an accuracy of 0.7045. Age was found to be the most important predictor of diabetes followed by bmi and weight.
 
## Constructing a SVC model using the training dataset

```{r}
set.seed(123)

### Setting a 10 fold CV,
train_control<-trainControl(method="cv", number=10)


### Scaling the data and Incorporating different values for cost (C) to train the model

svm.diabetes<-train(diabetes ~ ., data=train.data, method="svmLinear",  trControl=train.control, preProcess=c("center", "scale"), tuneGrid=expand.grid(C=seq(0.001,2, length=30)))

### Visualizing accuracy versus values of C
plot(svm.diabetes)  

### information about final model
svm.diabetes$results
svm.diabetes$finalModel

### Obtaining metrics of accuracy from training
confusionMatrix(svm.diabetes)

```
The final model has cost C = 1.93 and an accuracy of 0.7126 (#29). However it can also be observed that in #6 the accuracy is almost similar (0.7106) for a much lower cost C of 0.34. 

## Constructing a logistic regression model

```{r}
set.seed(123)


### training model with logistic regression (regularized)

logreg.diabetes <- train(
  diabetes ~ ., 
  data = train.data, 
  method = "glm", 
  trControl = train.control,
  family = "binomial",
  preProcess=c("center", "scale")
  
)


### Printing results
logreg.diabetes$results
confusionMatrix(logreg.diabetes)



```
The logistic regression model had anaccuracy of 0.7148 and a kappa of 0.23. Upon comparing the accuracies of all the 3 models, the logistic regression model had the highest accuracy and hence I will choose **logreg.diabetes**as my final model.

## Generating predictions in the test data

```{r}

#Applying model in test set to generate predictions and construct evaluation metrics

#First create predictions
pred.log<-predict(logreg.diabetes, newdata=test.data)
#Then use postResample to obtain evaluation metrics
confusionMatrix(pred.log, test.data$diabetes, positive="Yes")
```
## Reporting final evaluation Metrics:

After applying the logistic regression model in the test dataset, the accuracy was 0.7093 and Sensitivity was 0.80

## Limitations:

1) Interpretability: Support Vector Classifiers and classification trees have higher interpretability as compared to logistic regression models and thus it has limited interpretability. Additionally we won't be able to identuf the predictors that are more important to predict diabetes.

2) Since we used down-sampling for the training data and we did not balance the test data it may not accurately reflect the performance of the model in the real world.

